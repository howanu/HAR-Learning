---
title: "Learning Human Activity Recognition"
author: "Howanu"
date: "Saturday, November 22, 2014"
output: html_document
---

```{r settings, message=FALSE, echo=FALSE}
require(knitr)
# digits doesn't work
opts_chunk$set(echo=FALSE,message=FALSE,digits=7)
```

```{r libraries}
require(ggplot2)
require(plyr)
require(kfigr)
require(caret)
require(ROCR)
```


```{r read, cache=TRUE}
rawtr <- read.csv('pml-training.csv', stringsAsFactors=FALSE)
rawts <- read.csv('pml-testing.csv', stringsAsFactors=FALSE)
rawclassendx <- which(colnames(rawtr)=="classe")
```

```{r clean}
# Make sure the train and test sets have the same features
stopifnot(all.equal(colnames(rawtr[,-rawclassendx])
                    , colnames(rawts[,-rawclassendx])))

# Note that we look at the testing set.
# If you can't test against it, you should not model against it
missing_colndx <- sapply(1:ncol(rawts), function(c) {
  sum(is.na(rawts[,c])) != 0
  })
missing_variables_removed <- colnames(rawts)[which(missing_colndx)]

# Remove timestamps and user names
inappropriate_colndx <- grep(pattern = 'X|window|timestamp|user_name'
                             , colnames(rawtr))
inappropriate_variables_removed <- colnames(rawts)[inappropriate_colndx]

tr <- rawtr[, - c(which(missing_colndx), inappropriate_colndx)]
ts <- rawts[, - c(which(missing_colndx), inappropriate_colndx)]
```

```{r separate-training}
test_fraction <- 0.40
tsndx <- createDataPartition(tr$classe, p = test_fraction,list=FALSE)
trtr <- tr[-tsndx,]
trts <- tr[tsndx,]
```

```{r data-forest}

found_best <- TRUE # Runs overnight; let's re-use the best one
if (! found_best) {
    ## 10-fold CV
    fitControl <- trainControl(
    method = "repeatedcv"
    , number = 10
    , repeats = 10) ## repeated ten times
  set.seed(8181)
  fit <- train(as.factor(classe)~.
               , method = "rf" 
               , data=trtr
               , trControl = fitControl) 
} else {
  load("fit.RData")
}
```

```{r testing}
trtsresults <- predict(fit, newdata=trts)
```

```{r submission}
answers <- as.character(predict(fit,newdata=ts))
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
    }
  }
if (! found_best) {
  pml_write_files(answers)
}
```

## Model
Data were fit to a Random Forest model, built using 10-fold cross validation repeated 10 times. Training was done on 60% of the given training data, on all of the data features retained after cleaning (See Analysis Choices below).

## Cross Validation

The choice of ten-fold cross validation and ten repititions was somewhat arbitrary, but was chosen based on the volume of data and the model training time. Cross validation in general was chosen in order to reduce out-of-sample bias and to improve the out-of-sample error estimation accuracy.

The training set, after partitioning to set aside a testing sample, has `r nrow(trtr)` observations. 10-fold validation therefore yields `r as.numeric(0.9 * nrow(trtr))` observations in each training fold, which seems large enough not to compromise training accuracy.

The run time was a practical consideration. 10-fold validation ran overnight. This limited the number of iterations that could be done to vary parameters and improve the model. 

## Out-of-sample Error

Although the random forest authors [claim](http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#ooberr) that "there is no need for cross-validation or a separate test set to get an unbiased estimate of the test set error", I did choose to both cross validate and also to partition the training data into a model training and a test set. I chose cross validation to reduce the risk of overfitting, and I chose to partition the data in order to improve the estimate of of out-of-sample error.

The random forest model was built with `r 100*(1-test_fraction)`% of the given training set.

```{r oob}
# Obtains metrics for the best fit
oob_metrics <- fit$results[fit$results$mtry==fit$bestTune$mtry[1],]
```

The out-of-bag error was `r 1 - oob_metrics$Accuracy`, with accuracy within a 95% confidence interval of (`r oob_metrics$Accuracy + c(-1,1) * 1.96 * oob_metrics$AccuracySD`), assuming normally distributed error. However, out-of-bag error may reflect overfitting. A more realistic error measure was obtained through testing of the data partition that did not participate in training, as discussed below.

```{r ROC, anchor="figure"}
trtsresults.probA <- predict(fit,type="prob",newdata=trts)$A
A_or_notA <- as.factor(ifelse(trts$classe=="A", "A", "notA"))

pred <- prediction(trtsresults.probA, A_or_notA, label.ordering = c("notA", "A"))
perf <- performance(pred,"tpr","fpr")
plot(perf,main="ROC for Correct versus Incorrect Form Predictions\nFrom Observations Not Used in Training",col=2,lwd=2)
abline(a=0,b=1,lwd=2,lty=2,col="gray")
```

THe 40% of the training data that did not participate in random forest training was used to measure out-of-sample error. The Receiver Operating Characteristic (ROC) curve above shows the performance of the random forest model on prediction of correct weighlifting form (classe=="A"). This is an easier classification problem than identifying the type of incorrect weightlifting form. The area under the ROC curve is `r unlist(slot(performance(pred,"auc"), "y.values"))` (actually 0.9999006). We can pick a threshold with both extremely low false positives and extremely high true positives.

The accuracy of the prediction for all classes (the harder classification problem) is `r round(100.0 * sum(trtsresults==trts$classe)/length(trtsresults),1)`%.
 
## Analysis Choices

* Features with any missing values in the test set were removed from both the test and training sets (`r figr('missing_variables_removed', type="figure", link=TRUE)`), because in general the model might require the missing variable in order to classify a test observation. 
* Features that would not be known in a true prediction scenario, or that can not realistically be predictors, were also removed. For example, timestamps are not valid predictors because prediction should be possible in the future, when test timestamps will be different from those in the training set. Similarly, time window identification features may exactly predict observations that were done during the initial experiment, but will surely fail in a future prediction scenario. (`r figr('inappropriate_variables_removed', type="figure", link=TRUE)`)

## Assignment Observations

This assignment has offered lessons on the role of the data scientist. Given the features provided, random forest easily created an almost perfect classifier with practically no effort. Although some credit is due to the algorithm, and some to the ease of use of the R ecosystem, I believe most of the credit is due to the features. The authors of the data set performed valuable feature extraction by selection of optimal time windows and extraction of window-based features.

This assignment suggests that data scientist can contribute to a successful model using nothing but technical data science know-how. In this case, it seems model fitting requires no understanding of the meaning of features. However, I suspect that the valuable feature extraction upon which this assignment was based required the authors' data science expertise in combination with an understanding of the weightlifting domain, the meaning of the raw features, and the experiment.

Indeed, data science may be the most tractable aspect of hard modeling problems. Domain and experimental expertise are probably harder and time-consuming to master. With recent improvements in data science tools, increased hardware performance, and corresponding improvements in data science techniques, data analysis may be within closer reach of the average practioner than the other pieces of the modeling effort.

## References
* Data was made available through the [Human Activity Recognition](http://groupware.les.inf.puc-rio.br/har) project:

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

Read more: http://groupware.les.inf.puc-rio.br/har#ixzz3JpxuN665


* Training data: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv
* Testing data: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

## Data Set License

**Important**: you are free to use this dataset for any purpose. **This dataset is licensed under the Creative Commons license (CC BY-SA)**. The CC BY-SA license means you can remix, tweak, and build upon this work even for commercial purposes, as long as you credit the authors of the original work and you license your new creations under the identical terms we are licensing to you. This license is often compared to "copyleft" free and open source software licenses. All new works based on this dataset will carry the same license, so any derivatives will also allow commercial use.

## Appendix

### Missing features removed
```{r missing_variables_removed, anchor="figure", comment=NA, echo=TRUE}
missing_variables_removed
```

### Inappropriate features removed
```{r inappropriate_variables_removed, anchor="figure", comment=NA, echo=TRUE}
inappropriate_variables_removed
```




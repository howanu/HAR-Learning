---
title: "Learning Human Activity Recognition"
author: "Howanu"
date: "Saturday, November 22, 2014"
output: html_document
---

```{r settings, message=FALSE, echo=FALSE}
require(knitr)
# digits doesn't work
opts_chunk$set(echo=FALSE,message=FALSE,digits=7)
```

```{r libraries}
require(ggplot2)
require(plyr)
require(kfigr)
require(caret)
require(ROCR)
```


```{r read, cache=TRUE}
rawtr <- read.csv('pml-training.csv', stringsAsFactors=FALSE)
rawts <- read.csv('pml-testing.csv', stringsAsFactors=FALSE)
rawclassendx <- which(colnames(rawtr)=="classe")
```

```{r clean}
# Make sure the train and test sets have the same features
stopifnot(all.equal(colnames(rawtr[,-rawclassendx])
                    , colnames(rawts[,-rawclassendx])))

# Note that we look at the testing set.
# If you can't test against it, you should not model against it
missing_colndx <- sapply(1:ncol(rawts), function(c) {
  sum(is.na(rawts[,c])) != 0
  })
missing_variables_removed <- colnames(rawts)[which(missing_colndx)]

# Remove timestamps and user names
inappropriate_colndx <- grep(pattern = 'X|window|timestamp|user_name'
                             , colnames(rawtr))
inappropriate_variables_removed <- colnames(rawts)[inappropriate_colndx]

tr <- rawtr[, - c(which(missing_colndx), inappropriate_colndx)]
ts <- rawts[, - c(which(missing_colndx), inappropriate_colndx)]
```

```{r separate-training}
test_fraction <- 0.40
tsndx <- createDataPartition(tr$classe, p = test_fraction,list=FALSE)
trtr <- tr[-tsndx,]
trts <- tr[tsndx,]
```

```{r data-forest}

found_best <- TRUE # Runs overnight; let's re-use the best one
if (! found_best) {
    ## 10-fold CV
    fitControl <- trainControl(
    method = "repeatedcv"
    , number = 10
    , repeats = 10) ## repeated ten times
  set.seed(8181)
  fit <- train(as.factor(classe)~.
               , method = "rf" 
               , data=trtr
               , trControl = fitControl) 
} else {
  load("fit.RData")
}
```

```{r testing}
trtsresults <- predict(fit, newdata=trts)
```

```{r submission}
answers <- as.character(predict(fit,newdata=ts))
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
    }
  }
if (! found_best) {
  pml_write_files(answers)
}
```

## Model
Data were fit to a Random Forest model, built using 10-fold cross validation repeated 10 times. Training was done on all of the data features retained after cleaning (See Analysis Choices below).

## Cross Validation

The choice of ten-fold cross validation was somewhat arbitrary, but was chosen based on the volume of data and the model training time. K-fold cross validation was chosen in order to reduce out-of-sample bias and to improve the out-of-sample error estimation accuracy.

The training set, after partitioning to set aside a testing sample, has `r nrow(trtr)` observations. 10-fold validation therefore yields `r round(0.9 * nrow(trtr),0)` observations in each training fold, which seems large enough not to compromise training accuracy.
The run time was a practical consideration. 10-fold validation ran overnight. This limited the number of iterations that could be done to vary parameters and improve the model. 

## Out-of-sample Error

Although the random forest authors [claim](http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#ooberr) that "there is no need for cross-validation or a separate test set to get an unbiased estimate of the test set error", I did choose to both cross validate and also to partition the training data into a model training and a test set. I chose cross validation to reduce the risk of overfitting, and I chose to partition the data in order to improve the estimate of of out-of-sample error.

The random forest model was built with `r 100*(1-test_fraction)`% of the given training set.

```{r oob}
# Obtains metrics for the best fit
oob_metrics <- fit$results[fit$results$mtry==fit$bestTune$mtry[1],]
```

The out-of-bag error was `r 1 - oob_metrics$Accuracy`, with accuracy within a 95% confidence interval of (`r oob_metrics$Accuracy + c(-1,1) * 1.96 * oob_metrics$AccuracySD`), assuming normally distributed error. However, out-of-bag error may reflect overfitting. A more realistic error measure can be obtained through testing of the data partition that did not participate in training.

```{r ROC, anchor="figure"}
trtsresults.probA <- predict(fit,type="prob",newdata=trts)$A
A_or_notA <- as.factor(ifelse(trts$classe=="A", "A", "notA"))

pred <- prediction(trtsresults.probA, A_or_notA, label.ordering = c("notA", "A"))
perf <- performance(pred,"tpr","fpr")
plot(perf,main="ROC for Correct versus Incorrect Form Predictions\nFrom Observations Not Used in Training",col=2,lwd=2)
abline(a=0,b=1,lwd=2,lty=2,col="gray")
```

The Receiver Operating Characteristic (ROC) curve above shows the performance of the random forest model on prediction of correct weighlifting form (classe=="A"). The area under the ROC curve is `r unlist(slot(performance(pred,"auc"), "y.values"))` (actually 0.9999006). The accuracy of the prediction for all classes (not just class A) is `r round(100 * sum(trtsresults==trts$classe)/length(trtsresults),1)`%

 
## Analysis Choices

* Features with any missing values in the test set were removed from both the test and training sets (`r figr('missing_variables_removed', type="figure", link=TRUE)`), because in general the model might require the missing variable in order to classify a test observation. 
* Features that would not be known in a true prediction scenario, or that can not realistically be predictors, were also removed. For example, timestamps are not valid predictors because prediction should be possible in the future, when test timestamps will be different from those in the training set. Similarly, time window identification features may exactly predict observations that were done during the initial experiment, but will surely fail in a future prediction scenario. (`r figr('inappropriate_variables_removed', type="figure", link=TRUE)`)

## References
* Data was made available through the [Human Activity Recognition](http://groupware.les.inf.puc-rio.br/har) project:

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

Read more: http://groupware.les.inf.puc-rio.br/har#ixzz3JpxuN665


* Training data: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv
* Testing data: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

## Data Set License

**Important**: you are free to use this dataset for any purpose. **This dataset is licensed under the Creative Commons license (CC BY-SA)**. The CC BY-SA license means you can remix, tweak, and build upon this work even for commercial purposes, as long as you credit the authors of the original work and you license your new creations under the identical terms we are licensing to you. This license is often compared to "copyleft" free and open source software licenses. All new works based on this dataset will carry the same license, so any derivatives will also allow commercial use.

## Appendix

### Missing features removed
```{r missing_variables_removed, anchor="figure", comment=NA, echo=TRUE}
missing_variables_removed
```

### Inappropriate features removed
```{r inappropriate_variables_removed, anchor="figure", comment=NA, echo=TRUE}
inappropriate_variables_removed
```



